import numpy as np 
from nnfs.datasets import spiral_data


# Vamos começar com a implementação de uma rede neural simples
# com camadas densas (fully connected layers), funções de ativação
# ReLU e SoftMax.

class Layer_densa():
    def __init__(self, n_inputs, n_neuronios):
        # Inicializando os pesos (weights) com valores aleatórios a partir de uma distribuição normal
        # e os vieses (biases) com zeros. 
        # n_inputs: número de entradas (features) que a camada recebe.
        # n_neuronios: número de neurônios nesta camada.
        self.weights = 0.1*np.random.randn(n_inputs, n_neuronios)  # Peso inicial pequeno
        self.biases = np.zeros((1, n_neuronios))  # Bias inicializado com zero
    
    def foward(self, inputs):
        # A operação 'forward pass' da camada: multiplicação de inputs pelos pesos + bias
        # np.dot faz o produto escalar entre inputs e pesos, e somamos os biases.
        self.output = np.dot(inputs, self.weights) + self.biases  # Calculando a saída da camada

# Função de ativação ReLU (Rectified Linear Unit)
class ReLu():
    def foward(self, inputs):
        # A função ReLU retorna o maior valor entre o input e 0, 
        # ou seja, ela "zera" todos os valores negativos.
        # Essa função introduz não linearidade no modelo.
        self.output = np.maximum(0, inputs)  # Se a entrada for negativa, torna-se 0; caso contrário, permanece igual.
        
# Função de ativação SoftMax
class SoftMax():
    def foward(self, inputs):
        # A função SoftMax é usada para converter os valores de saída em uma distribuição de probabilidade.
        # Isso é feito aplicando a exponencial aos inputs e normalizando a soma dos resultados para 1.
        
        # Subtraímos o valor máximo de cada input para evitar problemas numéricos (overflow).
        exp_valores = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))  
        
        # Normalizamos os valores exponenciais para que a soma de cada linha seja igual a 1.
        probabilidades = exp_valores / np.sum(exp_valores, axis=1, keepdims=True)
        
        # A saída é uma distribuição de probabilidades entre as classes.
        self.output = probabilidades

# Gerando dados de exemplo para o treinamento, usando a função spiral_data.
# Aqui, X são as entradas e y são as saídas correspondentes.
X, y = spiral_data(samples=100, classes=3)  # Gerando 100 amostras de 3 classes, forma espiral

# Criando a primeira camada densa: 2 entradas (features) e 3 neurônios na camada.
layer1 = Layer_densa(2, 3)

# Criando a função de ativação ReLU para a primeira camada.
activation1 = ReLu()

# Criando a segunda camada densa: 3 entradas (da camada anterior) e 3 neurônios na camada.
layer2 = Layer_densa(3, 3)

# Criando a função de ativação SoftMax para a saída final.
activation2 = SoftMax()

# Passando os dados de entrada pela primeira camada, seguido pela ativação ReLU
layer1.foward(X)  
activation1.foward(layer1.output)

# Passando a saída da primeira camada pela segunda camada, seguido pela ativação SoftMax
layer2.foward(activation1.output)  
activation2.foward(layer2.output)

# Imprimindo as primeiras 5 previsões (probabilidades de cada classe)
# Cada linha representa a distribuição de probabilidade para uma amostra.
print(activation2.output[:5])




